# ðŸ§  Medical Self-Supervised Learning

## ðŸ“š Resources

### ðŸ”— Repositories
- [**3D DINO (AIM)**](https://github.com/AIM-Harvard/DINOv2-3D-Med) â€” 3D implementation of DINOv2  
- [**3D DINO (AICONS)**](https://github.com/AICONSlab/3DINO) â€” 3D implementation of DINOv2
- [**foundation based registration**](https://github.com/mazurowski-lab/Foundation-based-reg) â€” training-free (zero-shot) medical image registration pipeline using vision foundation models as feature encoders  
- [**nnssl pre-training**](https://github.com/MIC-DKFZ/nnssl?tab=readme-ov-file#complimentary-resources) â€” 3D implementation of strong pre-training methods using both CNN and Transformer architectures based on nnU-Net framework  
- [**nnssl fine-tuning**](https://github.com/TaWald/nnUNet) â€” downstream segmentation pretraining and adaptation framework based on nnU-Net framework  

---

## ðŸš€ Research Directions
- **Multimodal CT-MRI pre-training** using Multi-MAE approach  
- **Bilateral dense contrastive learning**  
- **Registration as a pretext task** for self-supervised pre-training  
- **CroCo-based self-supervised pre-training** for improving registration downstream tasks  
